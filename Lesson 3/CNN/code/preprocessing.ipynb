{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocessing.ipynb","version":"0.3.2","provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"8Y7ZZu-0vndq","colab_type":"code","outputId":"ead058ea-e2ca-4354-b31e-402d5cf2ba93","executionInfo":{"status":"ok","timestamp":1565315941923,"user_tz":-420,"elapsed":1806,"user":{"displayName":"Mar Heaven","photoUrl":"https://lh4.googleusercontent.com/-yP3GNUgefoI/AAAAAAAAAAI/AAAAAAAAABc/BTjJt5MEJz0/s64/photo.jpg","userId":"09945430610895316219"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"02quBEBawTmj","colab_type":"code","outputId":"e4cde296-b4b7-48ec-ce77-0577caec601d","executionInfo":{"status":"ok","timestamp":1565315944895,"user_tz":-420,"elapsed":4763,"user":{"displayName":"Mar Heaven","photoUrl":"https://lh4.googleusercontent.com/-yP3GNUgefoI/AAAAAAAAAAI/AAAAAAAAABc/BTjJt5MEJz0/s64/photo.jpg","userId":"09945430610895316219"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","import gensim as gs\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","import keras.models\n","from keras.models import Sequential\n","from keras.layers import Activation, Dense, Dropout\n","from sklearn.model_selection import cross_val_score, GridSearchCV\n","from sklearn.svm import SVC\n","from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot\n","import math\n","import re\n","import csv\n","import gensim\n","from gensim.models import Word2Vec\n","import gensim.models.keyedvectors as word2vec"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"5bdolT8QwXc3","colab_type":"code","colab":{}},"source":["\n","def read_data(filename):\n","    file = []\n","    with open(filename,encoding=\"utf8\") as fp:\n","        line = fp.readline()\n","#         line = gs.utils.simple_preprocess(line)\n","        while line:\n","           file.append(line)\n","           line = fp.readline()\n","#            line = gs.utils.simple_preprocess(line)\n","    file = np.reshape(file, (len(file),1))\n","    fp.close()\n","    return file\n","\n","def load_train_data(filepath, minidx, maxidx):\n","    filename = filepath + str(minidx) +'.txt'\n","    file = read_data(filename)\n","    data = np.array(file)\n","    for i in range(minidx +1 , maxidx +1 ):\n","        filename = filepath + str(i) +'.txt'\n","        file = read_data(filename)\n","        data = np.concatenate((data,file), axis=0)\n","\n","    labels = []\n","    for i in range(minidx, maxidx +1):\n","        labels += [i]*2000\n","\n","    return data, labels\n","\n","\n","def load_test_data(filepath):\n","    filename_1 = filepath +  'data.txt'\n","    file = read_data(filename_1)\n","    data = np.array(file)\n","\n","    filename_2 = filepath + 'label.txt'\n","    with open(filename_2) as f:\n","        labels = []\n","        for line in f:\n","            labels.append(int(line))\n","\n","    f.close()\n","    labels = np.array(labels)\n","    return data, labels\n","  \n","def one_hot_coding(Y,num_class):\n","    Y_one_hot_code = np.zeros((len(Y),num_class))\n","    for i in np.arange(len(Y)):\n","        Y_one_hot_code[i][Y[i]-1] = 1\n","    return Y_one_hot_code    \n","  \n","def pre_process(text):\n","    text = text.lower()\n","    # remove tags\n","    text = re.sub(\"<!--?.*?-->\", \"\", text)\n","    # remove special characters and digits\n","    text = re.sub(\"(\\\\d|\\\\W)+\", \" \", text)\n","    text = text.split()\n","    return text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TnQcchf2xqVr","colab_type":"code","colab":{}},"source":["def get_split_word(data):\n","  doc_list = []\n","  for d in data:\n","    doc_list.append(pre_process(d[0]))\n","  return doc_list"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Ne7lVkVwhOY","colab_type":"code","colab":{}},"source":["data_train, labels_train = load_train_data('/content/drive/My Drive/Intership/classify_data/train/', 1, 13)\n","data_test, labels_test = load_test_data('/content/drive/My Drive/Intership/classify_data/test/')\n","data = np.concatenate([data_train, data_test], axis = 0 )\n","\n","data = get_split_word(data)\n","# print(data_train)\n","# X_train, X_test = feature_engineering(data_train,data_test)\n","# print(X_train.shape)\n","# print(X_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2BarXr-zNv4","colab_type":"code","colab":{}},"source":["file_model = '/content/drive/My Drive/Intership/Week2/CNN/model/'\n","model = Word2Vec(data, size=256, window=5, min_count=0, workers=4, sg=1)\n","model.wv.save(file_model + 'preprocessing_256.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SekTl2utYymt","colab_type":"code","colab":{}},"source":["model['lê_duy_ứng']"],"execution_count":0,"outputs":[]}]}